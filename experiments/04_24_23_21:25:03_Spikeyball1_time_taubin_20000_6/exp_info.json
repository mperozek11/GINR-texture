{"model": "MLP(\n  (model): ModuleList(\n    (0): Linear(in_features=101, out_features=512, bias=True)\n    (1): Sine()\n    (2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (3): Linear(in_features=512, out_features=512, bias=True)\n    (4): Sine()\n    (5): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (6): Linear(in_features=512, out_features=512, bias=True)\n    (7): Sine()\n    (8): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (9): Linear(in_features=512, out_features=512, bias=True)\n    (10): Sine()\n    (11): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (12): Linear(in_features=613, out_features=512, bias=True)\n    (13): Sine()\n    (14): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (15): Linear(in_features=512, out_features=512, bias=True)\n    (16): Sine()\n    (17): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (18): Linear(in_features=512, out_features=512, bias=True)\n    (19): Sine()\n    (20): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (21): Linear(in_features=512, out_features=36, bias=True)\n  )\n)", "optimizer": "Adam (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    eps: 1e-08\n    foreach: None\n    lr: 0.0001\n    maximize: False\n    weight_decay: 0\n)", "loss_fn": "MSELoss()", "epochs": 60, "rand_inits": 1, "rand_seed": 11, "experiment notes": "first surface norms experiment", "best_rand_init": 0, "train_results": [{"run": 0, "best_loss": 0.0242297500371933, "train_losses": [1.0931093909523704, 1.0010431463068181, 0.9181917363947089, 0.8378109498457476, 0.7591992291537198, 0.6874001242897727, 0.6201488754966042, 0.5575233806263317, 0.509455680847168, 0.46449102054942737, 0.414737961509011, 0.369216572154652, 0.33405124057422986, 0.30008433081886987, 0.26435125957835803, 0.23295972564003684, 0.20674352212385697, 0.184424335306341, 0.1685367931019176, 0.1465462879701094, 0.13068485260009766, 0.12383602965961803, 0.10440764643929222, 0.09444607387889516, 0.08562844449823553, 0.07471181045879018, 0.06619089299982245, 0.05974474820223721, 0.053407473997636276, 0.04913051561875777, 0.045036814429543236, 0.044031652537259186, 0.040096770633350716, 0.03748992085456848, 0.035120048306205055, 0.033747312697497284, 0.031645728783173996, 0.030047652396288784, 0.029101767323233864, 0.028466907414523037, 0.027693466706709427, 0.027176832610910587, 0.027173668146133423, 0.026666367595846004, 0.02589321407404813, 0.025915237990292637, 0.025508414615284313, 0.025130903178995304, 0.025419557636434383, 0.025253019549629906, 0.025107771158218384, 0.024918726899407127, 0.024691635912114925, 0.024667550217021595, 0.024460128762505272, 0.024439676241441208, 0.024320602416992188, 0.02417706088586287, 0.024231945926492863, 0.023963884873823685], "eval_losses": [1.0479841232299805, 0.9663591980934143, 0.8652905225753784, 0.808639407157898, 0.7350425124168396, 0.6451559066772461, 0.6036766767501831, 0.5116844177246094, 0.5050391554832458, 0.4432343542575836, 0.4030027985572815, 0.34905123710632324, 0.29986193776130676, 0.26855990290641785, 0.2395726889371872, 0.21548525989055634, 0.18918254971504211, 0.17308998107910156, 0.14849229156970978, 0.13685494661331177, 0.1238604187965393, 0.09987086802721024, 0.10047001391649246, 0.09595990926027298, 0.08261596411466599, 0.07277767360210419, 0.06583631038665771, 0.05593659728765488, 0.0499856173992157, 0.0472392812371254, 0.04499273747205734, 0.043004438281059265, 0.03929079324007034, 0.038003601133823395, 0.03050142526626587, 0.030275506898760796, 0.027423761785030365, 0.030388908460736275, 0.03017004206776619, 0.027451124042272568, 0.02952820435166359, 0.02726692147552967, 0.026684362441301346, 0.026586584746837616, 0.025524912402033806, 0.02479548379778862, 0.02545342780649662, 0.026754753664135933, 0.025471949949860573, 0.024645933881402016, 0.025609051808714867, 0.024544093757867813, 0.024057360365986824, 0.024812763556838036, 0.02433096431195736, 0.024222195148468018, 0.024266278371214867, 0.02447434328496456, 0.024258842691779137, 0.0242297500371933]}]}